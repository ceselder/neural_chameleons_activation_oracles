# Hugging Face token (required for gated models)
HF_TOKEN=your_token_here

# Base model to train chameleon on
BASE_MODEL=google/gemma-2-9b-it

# Activation Oracle model (must match base model architecture)
# See: https://huggingface.co/collections/adamkarvonen/activation-oracles
AO_MODEL=adamkarvonen/checkpoints_latentqa_cls_past_lens_addition_gemma-2-9b-it

# Layer to probe (typically ~50-70% through model)
# For gemma-2-9b-it (42 layers): layer 21 is ~50%
PROBE_LAYER=21

# Concept to test (must be in BENIGN_CONCEPTS)
# Options: German, HTML, Jokey, Mathematical, Comforting, Confused, Biology-focused,
#          All-caps, Literature-focused, Finnish, Chemistry-based
CONCEPT=German
